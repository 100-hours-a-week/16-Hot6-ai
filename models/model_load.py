import os
from transformers import BlipProcessor, BlipForConditionalGeneration

# ì›í•˜ëŠ” ì €ì¥ ê²½ë¡œ (ì´ ê²½ë¡œì— ë‹¨ì¼ íŒŒì¼ë¡œ ì €ì¥ë¨)
BASE_DIR = os.getcwd() # í˜„ì¬ ì„œë²„ì—ì„œ ì‹¤í–‰ë˜ëŠ” ê¸°ì¤€ ìœ„ì¹˜
SAVE_PATH = os.path.join(BASE_DIR, "models", "blip-base-merged")

# 1. ë‹¤ìš´ë¡œë“œ + ì €ì¥
print("[INFO] BLIP-base ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥ ì¤‘...")
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# 2. ë‹¨ì¼ íŒŒì¼ë¡œ ì €ì¥ (ê°€ì¤‘ì¹˜ 1ê°œë¡œ ë³‘í•©ë¨)
processor.save_pretrained(SAVE_PATH)
model.save_pretrained(SAVE_PATH, max_shard_size="10GB")

print("[âœ… DONE] ë‹¨ì¼ ê°€ì¤‘ì¹˜ íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ:", SAVE_PATH)

del processor
del model

# 3. txt2img ëª¨ë¸ ë¡œë“œ
# ai/models/model_load.py

from diffusers import StableDiffusionXLPipeline, AutoencoderKL


# ğŸ”§ ì €ì¥í•  ìœ„ì¹˜: ì‹¤í–‰ ìœ„ì¹˜ ê¸°ì¤€ ìƒëŒ€ê²½ë¡œ
MODEL_SAVE_DIR = os.path.join(BASE_DIR, "models", "sdxl")

# Hugging Face ëª¨ë¸ ID
BASE_MODEL_ID = "stabilityai/stable-diffusion-xl-base-1.0"
VAE_MODEL_ID = "madebyollin/sdxl-vae-fp16-fix"

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

print(f"[INFO] SDXL Base ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥ ì¤‘... â†’ {MODEL_SAVE_DIR}")
pipe = StableDiffusionXLPipeline.from_pretrained(
    BASE_MODEL_ID,
    torch_dtype="auto",
    variant="fp16",
    use_safetensors=True
)

# ëª¨ë¸ ì „ì²´ ì €ì¥ (config í¬í•¨)
pipe.save_pretrained(MODEL_SAVE_DIR, safe_serialization=True, max_shard_size="10GB")

print(f"[INFO] VAE ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥ ì¤‘... â†’ {MODEL_SAVE_DIR}")
vae = AutoencoderKL.from_pretrained(
    VAE_MODEL_ID,
    torch_dtype="auto"
)
vae.save_pretrained(MODEL_SAVE_DIR, safe_serialization=True, max_shard_size="10GB")

print(f"[âœ… DONE] SDXL + VAE ì €ì¥ ì™„ë£Œ: {MODEL_SAVE_DIR}")

